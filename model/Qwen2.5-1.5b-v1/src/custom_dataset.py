import pandas as pd
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, file_path, tokenizer, max_length=512):
        """
        Qwen2.5-1.5B-Instruct íŒŒì¸íŠœë‹ì„ ìœ„í•œ CustomDataset í´ë˜ìŠ¤
        Args:
            file_path (str): ë°ì´í„°ì…‹ CSV íŒŒì¼ ê²½ë¡œ
            tokenizer (AutoTokenizer): ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €
            max_length (int): ìµœëŒ€ í† í° ê¸¸ì´
        """
        # CSV ë°ì´í„° ë¡œë“œ
        self.data = pd.read_csv(file_path, encoding="utf-8-sig")

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ (ì§€ì‹œë¬¸)
        system_message = "ë‹¹ì‹ ì€ ê³ ê°ì´ ê´€ì‹¬ì„ ê°€ì§ˆ ë§Œí•œ ë§¤ë ¥ì ì¸ ì œí’ˆ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. ìƒí’ˆëª…ì— ë§ëŠ” ìƒí’ˆì„¤ëª…ì„ ìƒì„±í•´ì£¼ì„¸ìš”"

        # ë°ì´í„° í¬ë§· ë³€ê²½ (ChatML í˜•ì‹)
        self.data = [
            {
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": row["name"]},
                    {"role": "assistant", "content": row["words"]}
                ]
            }
            for _, row in self.data.iterrows()
        ]

        # tokenizer
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)  # ë°ì´í„° ê°œìˆ˜ ë°˜í™˜

    def __getitem__(self, idx):
        example = self.data[idx]

        # ğŸ”¹ ChatML í¬ë§·ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        chatml_prompt = self.tokenizer.apply_chat_template(
            example["messages"],
            tokenize=False,  # ë¨¼ì € ë¬¸ìì—´ í˜•íƒœë¡œ ë³€í™˜
            add_generation_prompt=False
        )

        # ğŸ”¹ í† í¬ë‚˜ì´ì§•
        encodings = self.tokenizer(
            chatml_prompt,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )

        input_ids = encodings["input_ids"].squeeze()
        attention_mask = encodings["attention_mask"].squeeze()
        labels = input_ids.clone()

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels  # Causal LMì˜ ê²½ìš° input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        }
