import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig

from trl import SFTTrainer

class Train:
    def __init__(self):
        base_model = "beomi/llama-2-ko-7b"  # ğŸ”¹ ì›ë³¸ ëª¨ë¸ ì‚¬ìš©

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,  # âœ… ì˜¬ë°”ë¥¸ ì„¤ì •
            bnb_4bit_compute_dtype=torch.float16,  # âœ… 4-bit ì—°ì‚° íƒ€ì… ì„¤ì •
            bnb_4bit_use_double_quant=True,  # âœ… ë”ë¸” ì–‘ìí™” ì ìš© (VRAM ì ˆì•½)
        )

        # ğŸ”¹ GPU ìƒíƒœ í™•ì¸
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"âœ… í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device == "cuda":
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            print(f"âœ… í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU: {torch.cuda.get_device_name(0)}")

        # ğŸ”¹ 4-bit ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            device_map="auto",
            quantization_config=bnb_config  # âœ… ì˜¬ë°”ë¥¸ ì„¤ì •
        )

        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = prepare_model_for_kbit_training(self.model)

        lora_config = LoraConfig(
            r=8,  # LoRA rank
            lora_alpha=32,  # LoRA alpha
            target_modules=["q_proj", "v_proj"],  # LoRA ì ìš©í•  ë ˆì´ì–´
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )

        self.model = get_peft_model(self.model, lora_config)
        print("âœ… ì–‘ìí™” ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ~")

    def model_train(self, epochs,
                    train_batch,
                    vali_batch,
                    train_dataset,
                    eval_dataset,
                    output_dir,
                    learning_rate):

        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=train_batch,
            per_device_eval_batch_size=vali_batch,
            save_steps=500,  # ğŸ”¹ ë” ì§§ì€ ê°„ê²©ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_total_limit=3,
            gradient_accumulation_steps=8,  # âœ… VRAM ìµœì í™” (8GB í™˜ê²½ ê³ ë ¤)
            logging_steps=100,
            eval_strategy="steps",
            learning_rate=learning_rate,
            eval_steps=500,
            logging_dir='./logs',
            fp16=True,  # âœ… `fp16=True`ë¡œ ë³€ê²½ (RTX 4070ì€ bf16 ë¯¸ì§€ì›)
            report_to="none",
            gradient_checkpointing=True,  # âœ… VRAM ì ˆì•½ (í•„ìˆ˜)
        )

        trainer = SFTTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizers=self.tokenizer
        )

        print(f"ğŸ”¹ Batch Size (Train): {training_args.per_device_train_batch_size}")
        print(f"ğŸ”¹ Batch Size (Eval): {training_args.per_device_eval_batch_size}")
        print(f"ğŸ”¹ Num Train Epochs: {training_args.num_train_epochs}")

        trainer.train()

        # ğŸ”¹ LoRA Adapter ì €ì¥ (bnb ëª¨ë¸ì€ LoRAë§Œ ì €ì¥ ê°€ëŠ¥)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        print(f"âœ… Fine-tuned LoRA ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")

