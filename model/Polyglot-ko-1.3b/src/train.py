import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from trl import SFTTrainer

class Train:
    def __init__(self):
        base_model = "EleutherAI/gemma-3-ko-1.3b"  # ğŸ”¹ ì›ë³¸ ëª¨ë¸ ì‚¬ìš©

        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,  # âœ… 4-bit ì–‘ìí™” ì ìš©
            bnb_4bit_compute_dtype=torch.float16,  # âœ… bf16ì´ ì•ˆ ë˜ë©´ float16 ì‚¬ìš©
            bnb_4bit_use_double_quant=True,  # âœ… VRAM ì ˆì•½ì„ ìœ„í•œ ë”ë¸” ì–‘ìí™”
        )

        # ğŸ”¹ GPU ìƒíƒœ í™•ì¸
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"âœ… í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {self.device}")
        if self.device == "cuda":
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            print(f"âœ… í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ GPU: {torch.cuda.get_device_name(0)}")

        # ğŸ”¹ 4-bit ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model,
            device_map="auto",
            quantization_config=bnb_config  # âœ… ì˜¬ë°”ë¥¸ ì„¤ì •
        )

        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        self.tokenizer.pad_token = self.tokenizer.eos_token  # âœ… íŒ¨ë”© í† í° ì„¤ì •

        # ğŸ”¹ LoRA ì ìš© ì „, k-bit ìµœì í™”
        self.model = prepare_model_for_kbit_training(self.model)

        lora_config = LoraConfig(
            r=8,  # LoRA rank
            lora_alpha=32,  # LoRA alpha
            target_modules=["query_key_value",],  # LoRA ì ìš©í•  ë ˆì´ì–´
            lora_dropout=0.1,
            bias="none",
            task_type="CAUSAL_LM"
        )

        self.model = get_peft_model(self.model, lora_config)
        print("âœ… ì–‘ìí™” ëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ~")

    def model_train(self, epochs,
                    train_batch,
                    vali_batch,
                    train_dataset,
                    eval_dataset,
                    output_dir,
                    learning_rate):

        # ğŸ”¹ TrainingArguments ì„¤ì • (ì˜¤íƒ€ ìˆ˜ì • ë° ìµœì í™”)
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=train_batch,
            per_device_eval_batch_size=vali_batch,
            save_steps=500,  # ğŸ”¹ 500 ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            save_total_limit=3,
            gradient_accumulation_steps=8,  # âœ… VRAM ìµœì í™” (RTX 4060 í™˜ê²½ ê³ ë ¤)
            logging_steps=100,
            evaluation_strategy="steps",  # âœ… `eval_strategy` â†’ `evaluation_strategy`
            learning_rate=learning_rate,
            eval_steps=500,
            logging_dir='./logs',
            fp16=True,  # âœ… RTX 4070ì€ bf16 ë¯¸ì§€ì› â†’ fp16 ì‚¬ìš©
            report_to="none",
        )

        trainer = SFTTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,  # âœ… Dataset ê°ì²´ê°€ ë“¤ì–´ê°€ì•¼ í•¨
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer  # âœ… `tokenizers=self.tokenizer` â†’ `tokenizer=self.tokenizer`
        )

        print(f"ğŸ”¹ Batch Size (Train): {training_args.per_device_train_batch_size}")
        print(f"ğŸ”¹ Batch Size (Eval): {training_args.per_device_eval_batch_size}")
        print(f"ğŸ”¹ Num Train Epochs: {training_args.num_train_epochs}")

        trainer.train()

        # ğŸ”¹ LoRA Adapter ì €ì¥ (bnb ëª¨ë¸ì€ LoRAë§Œ ì €ì¥ ê°€ëŠ¥)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        print(f"âœ… Fine-tuned LoRA ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
